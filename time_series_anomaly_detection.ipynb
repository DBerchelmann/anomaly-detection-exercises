{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from numpy import linspace, loadtxt, ones, convolve\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from random import randint\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "# style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "<h2>README</h2>\n",
    "\n",
    "- The following exercises show the process of utilizing functions to help analyze anomalies by using Exponential Moving Average and Bollinger Bands\n",
    "- The other goal for this exercise is to add doc-strings and markdown comments to describe the processes that are occurring within the function\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Steps to analyze time series anomalies\n",
    "\n",
    "     - Acquire the data\n",
    "     - Prep the data with user (observation)\n",
    "     - Compute the features\n",
    "         - compute mid-band\n",
    "         - compute standard deviation\n",
    "         - compute upper & lower band\n",
    "         - create a dataframe with metrics\n",
    "         - compute %b\n",
    "         - add the user_id to the dataframe\n",
    "     - Plot\n",
    "     - Search for anomalies\n",
    "     \n",
    "     \n",
    "- other items to consider\n",
    "    - Determine your K (also known as your 'weight'). Standard for real world is usually 6 (this is to reduce noise)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Acquire the data \n",
    "\n",
    "def acquire(file_name, column_names):\n",
    "    \n",
    "    '''This function acquires a file from a csv, uses no header (blank columns) then assigns column names & finishes by calling the columns to be pulled'''\n",
    "    \n",
    "    return pd.read_csv(file_name, sep='\\s', header=None, names=column_names, usecols=[0, 2, 3, 4, 5])\n",
    "\n",
    "# Prep the data\n",
    "\n",
    "def prep(df, user, span, weight):\n",
    "    \n",
    "    ''' This function uses the already acquired data frame and then preps the data by doing the following:\n",
    "         - have an argument where the user can be specified\n",
    "         - convert the date column to datetime\n",
    "         - setting the index as the date\n",
    "         - finally, this function will return a Pandas Series called pages that produces a user id with total pages accessed. '''\n",
    "    \n",
    "    df = df[df.user_id == user]\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "    df = df.set_index(df.date)\n",
    "    pages = df['endpoint'].resample('d').count()\n",
    "    \n",
    "# Let's compute the b percentage, %b\n",
    "\n",
    "def compute_pct_b(pages, span, weight, user):\n",
    "    \n",
    "    ''' This function calculates the midband, standard deviation, upper & lower bands. It then\n",
    "       concatanates the upper & lower bands together. After this is complete, the function then concatanates\n",
    "       the mid-band along with the upper/lower bands and pages which was created back in the prep function. New columns\n",
    "       are created to hold the 4 new columns that were brought together through concatanation. Finally, two new columns are \n",
    "       made which are the %b and user_id. The functions returns a new dataframe. '''\n",
    "    \n",
    "    midband = pages.ewm(span=span).mean()\n",
    "    stdev = pages.ewm(span=span).std()\n",
    "    ub = midband + stdev*weight\n",
    "    lb = midband - stdev*weight\n",
    "    bb = pd.concat([ub, lb], axis=1)\n",
    "    my_df = pd.concat([pages, midband, bb], axis=1)\n",
    "    my_df.columns = ['pages', 'midband', 'ub', 'lb']\n",
    "    my_df['pct_b'] = (my_df['pages'] - my_df['lb'])/(my_df['ub'] - my_df['lb'])\n",
    "    my_df['user_id'] = user\n",
    "    \n",
    "    return my_df\n",
    "\n",
    "# plot the upper, mid, lower bands for every user\n",
    "\n",
    "def plt_bands(my_df, user):\n",
    "    \n",
    "    ''' This functions will plot the upper, mid, lower bands for every user along with the original page counts'''\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    ax.plot(my_df.index, my_df.pages, label='Number of Pages, User: '+str(user))\n",
    "    ax.plot(my_df.index, my_df.midband, label = 'EMA/midband')\n",
    "    ax.plot(my_df.index, my_df.ub, label = 'Upper Band')\n",
    "    ax.plot(my_df.index, my_df.lb, label = 'Lower Band')\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_ylabel('Number of Pages')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Find anomalies\n",
    "\n",
    "def find_anomalies(df, user, span, weight):\n",
    "    \n",
    "    '''This function imports a clean/prepped data frame and also uses the user_id to calculate the percentage b by inputting the\n",
    "    span (the number of days for the exponential moving average) and the weight (that is our k which is used to reduce noise. The higher the k\n",
    "    the less the noise although, too high and no anomalies will be found). Finally, the function will return all instances of the pct_b column\n",
    "    that are over 1 '''\n",
    "    \n",
    "    pages = prep(df, user, span, weight)\n",
    "    my_df = compute_pct_b(pages, span, weight, user)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return my_df[my_df.pct_b>1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'logs_df.csv'\n",
    "column_names = ['date', 'endpoint', 'user_id', 'cohort_id', 'source_ip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire(file_name, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "prep() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-069981f02d87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muser_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_anomalies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0manomalies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c60213f1e68d>\u001b[0m in \u001b[0;36mfind_anomalies\u001b[0;34m(df, user, span, weight)\u001b[0m\n\u001b[1;32m     70\u001b[0m     that are over 1 '''\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mmy_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_pct_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: prep() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "span = 30\n",
    "weight = 3.5\n",
    "user_df = find_anomalies(df, user, span, weight)\n",
    "\n",
    "anomalies = pd.DataFrame()\n",
    "for u in list(df.user_id.unique()):\n",
    "    user_df = find_anomalies(df, u, span, weight)\n",
    "    anomalies = pd.concat([anomalies, user_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
